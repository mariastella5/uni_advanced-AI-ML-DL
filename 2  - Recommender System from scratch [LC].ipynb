{"cells":[{"cell_type":"markdown","metadata":{"id":"yVlwEEkWlXEu"},"source":["# LAB 2: Recommender System from Scratch"]},{"cell_type":"markdown","metadata":{"id":"aUSaHRazlauW"},"source":["Giorgio Lazzarinetti - My Contacts\n","For any questions or doubts you can find my contacts here:\n","\n","giorgiolazzarinetti@gmail.com g.lazzarinetti@campus.unimib.it"]},{"cell_type":"markdown","metadata":{"id":"Kg__F8YqsqGq"},"source":["## Notebook Outline\n","\n","- **Introduction to Recommender System**\n","- **Movielens Dataset**\n","- **Generalized Matrix Factorization Model**\n","- **Neural Collaborative Filtering**\n","- **LAB CHALLENGE 1: Neural Matrix Factorization**"]},{"cell_type":"markdown","metadata":{"id":"ZxysEMZCO5sp"},"source":["## References\n","\n","The architecture of the deep model, the evaluation strategy and the metrics used are taken from the paper: [\"Neural Collaborative Filtering\"](https://arxiv.org/abs/1708.05031) by He Xiangnan, Liao Lizi, Zhang Hanwang, Nie Liqiang, Hu Xia and Chua Tat-Seng - In Porc. of the 26th Interantional Conference on World Wide Web - 2017. "]},{"cell_type":"markdown","metadata":{"id":"vwK9BJ1bss0_"},"source":["## Introduction to Recommender System\n","\n","Recommender systems are algorithms that mimic the psychology and personality of humans, in order to predict their needs and desires. More formally, recommender systems adopt data-mining and machine-learning techniques to help users in finding attractive and useful products. Products can be almost anything: physical items (e.g., smartphones), places (e.g., restaurants), digital content (e.g., movies and music), and many more. Recommender systems produce recommendations based on different inputs: demographic information about users, ratings and comments on products, individual’s or community’s past preferences and choices, social networks, context of use.\n","\n","There are many different types of techniques and implementations out there.\n","\n","- **Content-based methods** uses attributes of items to recommend to users new items similar to what the user has liked in the past (doesn't take into account the behaviour of other users);\n","- **Collaborative Filtering methods** uses similarities between users and items simultaneously to determine recommendations;\n","- **Hybrid methos** mix content-based and collaborative filtering approaches.\n","\n","Other approaches are also called **Knowlege-based methods** which uses explicit knowledge about users and items to build recommendations criteria with a rule-based approach.\n","\n","\n","<center>  <img src=\"https://drive.google.com/uc?export=view&id=1Qaizz9YLvqgXg0blWFwN92IQSQPLTSoH\" width=\"950\" height=\"400\"> </center> \n","\n","\n","In the following we'll focus on Collaborative Filtering methods, with a model-based approach with deep learning algorithms.\n","\n","### Problem Definition\n","\n","Given a past record of movies seen by a user, we will build a recommender system that helps the user discover movies of their interest.\n","\n","Specifically, given <userID, itemID> occurrence pairs, we need to generate a ranked list of movies for each user.\n","\n","We model the problem as a binary classification problem, where we learn a function to predict whether a particular user will like a particular movie or not.\n","\n","$$f(userid, itemid) →, [0,1]$$\n","\n","The model takes in two sparse vectors, one representing the user and the other represents items. The users vector has size #users, while the items vector has size #items.  \n","\n","So, elaborately,\n","- User vector=[0,0,1...,0,0,0] with m elements, means this vector represents the 3 rd user out of m.\n","- Item vector=[0,1,0,0,0,0...0] with n elements, means this vector represents the 2 nd item out of n.\n","\n","Basically both items and users are one-hot encoded.\n","\n","These two vectors should be passed to a first embedding layer  (to project sparse representations to dense ones). These embeddings can be seen as a latent vector for users and items.\n","\n","Thus, the final predictive model will be\n","$$y_{ui} = f(\\mathbf{P}^T\\mathbf{v}_u^T, \\mathbf{Q}^T\\mathbf{v}_i^T | \\mathbf{P}, \\mathbf{Q}, \\mathbf{\\Theta}_f) $$\n","\n","where **P** and **Q** denotes the latent factor matrix for users and items and **$\\Theta_f$** denotes the model parameters. \n"]},{"cell_type":"markdown","metadata":{"id":"9131NU_plgbG"},"source":["### Packages"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"Yji9-7jslKRz"},"outputs":[],"source":["import os\n","import time\n","import random\n","import argparse\n","import numpy as np \n","import pandas as pd \n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.utils.data as data\n","from torch.utils.data import Dataset, DataLoader"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":240,"status":"ok","timestamp":1678180627507,"user":{"displayName":"Giorgio Lazzarinetti","userId":"02760930368358339868"},"user_tz":-60},"id":"arJu3Re7XCky","outputId":"6c75cf0e-068b-431a-fa1e-105685cc72cb"},"outputs":[{"data":{"text/plain":["<torch._C.Generator at 0x1b5651be070>"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["np.random.seed(7)\n","torch.manual_seed(0)"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"IkcVzJcY5TS_"},"outputs":[],"source":["use_cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")"]},{"cell_type":"markdown","metadata":{"id":"epmWgmA5w920"},"source":["## Movielens Dataset\n","\n","### Dataset\n","\n","We use the MovieLens 100K dataset, which has 100,000 ratings from 1000 users on 1700 movies. \n","\n","The ratings are given to us in form of <userID,itemID, rating, timestamp> tuples. Each user has a minimum of 20 ratings.\n","\n","You can download the dataset [here](https://grouplens.org/datasets/movielens/). Download the file ml-100k.zip. Unzip it and extract the file u.data \n","\n","### Create Dataset\n","\n","Here we are going to create the necessary dataset for building recommender system. \n","\n","After downloading the file as indicated, save them in your Colab Notebooks directory. In order to let the Notebook see the file on your Drive you have to mount it. "]},{"cell_type":"code","execution_count":4,"metadata":{"id":"qcffYFMdxDcI"},"outputs":[],"source":["dataset_origin = {'100k': 'u.data', '1M': 'ratings.dat'}\n","\n","num_sample_data = '100k'\n","#DATA_PATH = 'drive/MyDrive/Colab Notebooks/2 - Data/{}'.format(dataset_origin[num_sample_data]) #change this with your directory \n","DATA_PATH = 'data\\MovieLens/{}'.format(dataset_origin[num_sample_data]) #change this with your directory \n","MODEL_PATH = 'drive/MyDrive/Colab Notebooks/2 - Models/movielens_{}/'.format(num_sample_data) #change this with your directory \n"]},{"cell_type":"markdown","metadata":{"id":"kZ43K6UT9UmJ"},"source":["We now build some function to manage the data.\n","\n","We drop the exact value of rating (1,2,3,4,5) and instead convert it to an implicit scenario i.e. any positive interaction is given value of 1. All other interactions are given a value of zero, by default.\n","\n","Since we are training a classifier, we need both positive and negative samples. The records present in the dataset are counted as positive samples. We assume that all entries in the user-item interaction matrix are negative samples (a strong assumption, and easy to implement).\n","\n","We randomly sample 4 items that are not interacted by the user, for every item interacted by the user. This way, if a user has 20 positive interactions, he will have 80 negative interactions. These negative interactions cannot contain any positive interaction by the user, though they may not be all unique due to random sampling."]},{"cell_type":"markdown","metadata":{"id":"tE13kg0aydtu"},"source":["We now define the class MovieLens Dataset which will be used do read the data and create the train and test dataset. "]},{"cell_type":"code","execution_count":5,"metadata":{"id":"hmlijxEoYXDj"},"outputs":[],"source":["class Rating_Datset(Dataset):\n","\tdef __init__(self, user_list, item_list, rating_list):\n","\t\tsuper(Rating_Datset, self).__init__()\n","\t\tself.user_list = user_list\n","\t\tself.item_list = item_list\n","\t\tself.rating_list = rating_list\n","\n","\tdef __len__(self):\n","\t\treturn len(self.user_list)\n","\n","\tdef __getitem__(self, idx):\n","\t\tuser = self.user_list[idx]\n","\t\titem = self.item_list[idx]\n","\t\trating = self.rating_list[idx]\n","\t\t\n","\t\treturn (\n","\t\t\ttorch.tensor(user, dtype=torch.long),\n","\t\t\ttorch.tensor(item, dtype=torch.long),\n","\t\t\ttorch.tensor(rating, dtype=torch.float)\n","\t\t\t)"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"FxZCDy4tYkRZ"},"outputs":[],"source":["class NCF_Data(object):\n","\t\"\"\"\n","\tConstruct Dataset for NCF\n","\t\"\"\"\n","\tdef __init__(self, args, ratings):\n","\t\tself.ratings = ratings\n","\t\tself.num_ng = args.num_ng\n","\t\tself.num_ng_test = args.num_ng_test\n","\t\tself.batch_size = args.batch_size\n","\n","\t\tself.preprocess_ratings = self._reindex(self.ratings)\n","\n","\t\tself.user_pool = set(self.ratings['user_id'].unique())\n","\t\tself.item_pool = set(self.ratings['item_id'].unique())\n","\n","\t\tself.train_ratings, self.test_ratings = self._leave_one_out(self.preprocess_ratings)\n","\t\tself.negatives = self._negative_sampling(self.preprocess_ratings)\n","\n","\t\n","\tdef _reindex(self, ratings):\n","\t\t\"\"\"\n","\t\tProcess dataset to reindex userID and itemID, also set rating as binary feedback\n","\t\t\"\"\"\n","\t\tuser_list = list(ratings['user_id'].drop_duplicates())\n","\t\tuser2id = {w: i for i, w in enumerate(user_list)}\n","\n","\t\titem_list = list(ratings['item_id'].drop_duplicates())\n","\t\titem2id = {w: i for i, w in enumerate(item_list)}\n","\n","\t\tratings['user_id'] = ratings['user_id'].apply(lambda x: user2id[x])\n","\t\tratings['item_id'] = ratings['item_id'].apply(lambda x: item2id[x])\n","\t\tratings['rating'] = ratings['rating'].apply(lambda x: float(x > 0))\n","\t\treturn ratings\n","\n","\tdef _leave_one_out(self, ratings):\n","\t\t\"\"\"\n","\t\tleave-one-out evaluation protocol in paper https://www.comp.nus.edu.sg/~xiangnan/papers/ncf.pdf\n","\t\t\"\"\"\n","\t\tratings['rank_latest'] = ratings.groupby(['user_id'])['timestamp'].rank(method='first', ascending=False)\n","\t\ttest = ratings.loc[ratings['rank_latest'] == 1]\n","\t\ttrain = ratings.loc[ratings['rank_latest'] > 1]\n","\t\tassert train['user_id'].nunique()==test['user_id'].nunique(), 'Not Match Train User with Test User'\n","\t\treturn train[['user_id', 'item_id', 'rating']], test[['user_id', 'item_id', 'rating']]\n","\n","\tdef _negative_sampling(self, ratings):\n","\t\tinteract_status = (\n","\t\t\tratings.groupby('user_id')['item_id']\n","\t\t\t.apply(set)\n","\t\t\t.reset_index()\n","\t\t\t.rename(columns={'item_id': 'interacted_items'}))\n","\t\tinteract_status['negative_items'] = interact_status['interacted_items'].apply(lambda x: self.item_pool - x)\n","\t\tinteract_status['negative_samples'] = interact_status['negative_items'].apply(lambda x: random.sample(sorted(x), self.num_ng_test))\n","\t\treturn interact_status[['user_id', 'negative_items', 'negative_samples']]\n","\n","\tdef get_train_instance(self):\n","\t\tusers, items, ratings = [], [], []\n","\t\ttrain_ratings = pd.merge(self.train_ratings, self.negatives[['user_id', 'negative_items']], on='user_id')\n","\t\ttrain_ratings['negatives'] = train_ratings['negative_items'].apply(lambda x: random.sample(sorted(x), self.num_ng))\n","\t\tfor row in train_ratings.itertuples():\n","\t\t\tusers.append(int(row.user_id))\n","\t\t\titems.append(int(row.item_id))\n","\t\t\tratings.append(float(row.rating))\n","\t\t\tfor i in range(self.num_ng):\n","\t\t\t\tusers.append(int(row.user_id))\n","\t\t\t\titems.append(int(row.negatives[i]))\n","\t\t\t\tratings.append(float(0))  # negative samples get 0 rating\n","\t\tdataset = Rating_Datset(\n","\t\t\tuser_list=users,\n","\t\t\titem_list=items,\n","\t\t\trating_list=ratings)\n","\t\treturn DataLoader(dataset, batch_size=self.batch_size, shuffle=True, num_workers=4)\n","\n","\tdef get_test_instance(self):\n","\t\tusers, items, ratings = [], [], []\n","\t\ttest_ratings = pd.merge(self.test_ratings, self.negatives[['user_id', 'negative_samples']], on='user_id')\n","\t\tfor row in test_ratings.itertuples():\n","\t\t\tusers.append(int(row.user_id))\n","\t\t\titems.append(int(row.item_id))\n","\t\t\tratings.append(float(row.rating))\n","\t\t\tfor i in getattr(row, 'negative_samples'):\n","\t\t\t\tusers.append(int(row.user_id))\n","\t\t\t\titems.append(int(i))\n","\t\t\t\tratings.append(float(0))\n","\t\tdataset = Rating_Datset(\n","\t\t\tuser_list=users,\n","\t\t\titem_list=items,\n","\t\t\trating_list=ratings)\n","\t\treturn DataLoader(dataset, batch_size=self.num_ng_test+1, shuffle=False, num_workers=2)"]},{"cell_type":"markdown","metadata":{"id":"qx2UpNblDKH0"},"source":["### Evaluation Metrics\n","\n","We randomly sample 100 items that are not interacted by the user, ranking the test item among the 100 items. We truncate the ranked list at 10.\n","\n","Since it is too time-consuming to rank all items for every user, for we will have to calculate 1000\\*1700 ~10⁶ values. With this strategy, we need 1000*100 ~ 10⁵ values, an order of magnitude less.\n","\n","For each user, we use the latest rating(according to timestamp) in the test set, and we use the rest for training. This evaluation methodology is also known as leave-one-out strategy.\n","\n","#### Metrics\n","\n","We use **Hit Ratio** (HR), and **Normalized Discounted Cumulative Gain** (NDCG) to evaluate the performance for our RS.\n","\n","\n","In recommender settings, the **HR** is simply the fraction of users for which the correct answer is included in the recommendation list of length L.\n","\n","$$HR = \\frac{|U_{hit}^{L}|}{U_{all}}$$\n","\n","Where $U_{hit}^{L}$ is the number of users for which the correct answer is included in the top L recommendation list, while, $U_{all}$ is the total number of user in the test dataset. Clearly the larger L is the larger HR become.\n","\n","**NDCG** is a measure of ranking quality. In information retrieval, it is often used to measure effectiveness of web search engine algorithms or related applications. Using a graded relevance scale of documents in a search-engine result set, DCG measures the usefulness, or gain, of a document based on its position in the result list. The gain is accumulated from the top of the result list to the bottom, with the gain of each result discounted at lower ranks\n","\n","**Gain** for an item is essentialy the same as the relevance score, which can be numerical ratings like search results in Google which can be rated in scale from 1 to 5, or binary in case of implicit data where we only know if a user has consumed certain item or not. Naturally **Cumulative Gain** is defined as the sum of gains up to a position k in the recommendation list.\n","\n","$$CG(k) = \\sum_{i=1}^{k}G_i$$\n","\n","One obvious drawback of CG is that it does not take into account of ordering. By swapping the relative order of any two items, the CG would be unaffected. This is problematic when ranking order is important. For example, on Google Search results, you would obviously not like placing the most relevant web page at the bottom.\n","\n","To penalize highly relevant items being placed at the bottom, we introduce the **Discounted Cumulative Gaing** (DCG).\n","\n","$$DCG(k) = \\sum_{i=1}^{k} \\frac{G_i}{log_2(i+1)}$$\n","\n","By diving the gain by its rank, we sort of push the algorithm to place highly relevant items to the top to achieve the best DCG score.\n","\n","There is still a drawback of DCG score. It is that DCG score adds up with the length of recommendation list. Therefore, we cannot consistently compare the DCG score for system recommending top 5 and top 10 items, because the latter will have higher score not because its recommendation quality but pure length.\n","\n","We tackle this issue by introducing **Ideal Discounted Cumulative Gain** (IDCG). IDCG is the DCG score for the most ideal ranking, which is ranking the items top down according their relevance up to position k.\n","\n","$$IDCG(k) = \\sum_{i=1}^{|I(k)|} \\frac{G_i}{log_2(i+1)}$$\n","\n","Where $|I(k)|$ represent the ideal list of items up to position k.\n","\n","And NDCG is simply to normalize the DCG score by IDCG such that its value is always between 0 and 1 regardless of the length.\n","\n","$$NDCD(k) = \\frac{DCG(k)}{IDCG(k)}$$\n","\n","Our model gives a confidence score between 0 and 1 for each item present in the test set for a given user. The items are sorted in decreasing order of their score, and top 10 items are given as recommendation. If the test item (which is only one for each user) is present in this list, HR is one for this user, else it is zero. The final HR is reported after averaging for all users. A similar calculation is done for NDCG.\n","\n","While training, we will be minimizing the cross-entropy loss, which is the standard loss function for a classification problem. The real strength of RS lies in giving a ranked list of top-k items, which a user is most likely to interact. Think about why you mostly click on google search results only on the first page, and never go to other pages. Metrics like NDCG and HR help in capturing this phenomenon by indicating the quality of our ranked lists. "]},{"cell_type":"code","execution_count":7,"metadata":{"id":"uTaIX7H52i-a"},"outputs":[],"source":["def hit(ng_item, pred_items):\n","\tif ng_item in pred_items:\n","\t\treturn 1\n","\treturn 0\n","\n","\n","def ndcg(ng_item, pred_items):\n","\tif ng_item in pred_items:\n","\t\tindex = pred_items.index(ng_item)\n","\t\treturn np.reciprocal(np.log2(index+2))\n","\treturn 0\n","\n","\n","def metrics(model, test_loader, top_k, device):\n","\tHR, NDCG = [], []\n","\n","\tfor user, item, label in test_loader:\n","\t\tuser = user.to(device)\n","\t\titem = item.to(device)\n","\n","\t\tpredictions = model(user, item)\n","\t\t_, indices = torch.topk(predictions, top_k)\n","\t\trecommends = torch.take(\n","\t\t\t\titem, indices).cpu().numpy().tolist()\n","\n","\t\tng_item = item[0].item() # leave one-out evaluation has only one item per user\n","\t\tHR.append(hit(ng_item, recommends))\n","\t\tNDCG.append(ndcg(ng_item, recommends))\n","\n","\treturn np.mean(HR), np.mean(NDCG)"]},{"cell_type":"markdown","metadata":{"id":"KuigI6hAj6Fm"},"source":["## Generalized Matrix Factorization (GMF)\n","\n","Generally Matrix Factorization (MF) algorithms associates each user and item with a real-valued vector of latent features. \n","\n","Let $\\mathbf{p_u}$ and $\\mathbf{q_i}$ denote the latent vector for user u and item i, respectively; MF estimates an interaction y_{ui} as the inner product of $\\mathbf{p_u}$ and $\\mathbf{q_i}$:\n","\n","$$y_{ui} = f(u, i| \\mathbf{p_u}, \\mathbf{q_i}) = \\mathbf{p_u}^T\\mathbf{q_i} = \\sum_{k=1}^Kp_{uk}q_{ik} $$ \n","\n","Where K denotes the dimension of the latent space.\n","\n","MF models the dimension of the interaction of user and item latent factors, assuming each dimension of the latent space is independent of each other and linearly combining them  with the same weights. \n","\n","This imposes some limitation of MF caused by the use of a simple and fixed inner product to estimate complex user-item interactions in the low-dimensional latent space.\n","\n","In order to overcome this limitation, we can build a Generalized Matrix Factorization (GMF) algorithm where we can weight the linear combination of the element-wise product and use this with an activation function to learn a representation of the input insted of using a fixed one.\n","\n","Let the user latent vector $\\mathbf{p_u}$  be denoted as $\\mathbf{P}^T\\mathbf{v}_u^T$ and the item latent vector $\\mathbf{q_i}$ as $\\mathbf{Q}^T\\mathbf{v}_i^T$. \n","\n","The GMF can be expressed as \n","$$Y_{ui} = a_{out}(\\mathbf{h}^T(\\mathbf{p_u}\\odot \\mathbf{q_i})$$\n","where $a_{out}$ is the activation function and $\\mathbf{h}$ are the edge weights of the output layer. \n","\n","Intuitively, if we use an identity function as $a_{out}$ and enforce $\\mathbf{h}$ to be a uniform vector of 1, we can exactly recover the MF model.\n","\n","In the following we will use $\\mathbf{h}$ as linear layer and a $a_{out}$ as sigmoid function to learn $\\mathbf{h}$ weigths from data with the log loss.\n","\n"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"DTMHganmd5q1"},"outputs":[],"source":["class GMF(nn.Module):\n","    def __init__(self, args, num_users, num_items):\n","        super(GMF, self).__init__()\n","        self.num_users = num_users\n","        self.num_items = num_items\n","        self.factor_num = args.factor_num\n","\n","        self.embedding_user = nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.factor_num)\n","        self.embedding_item = nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.factor_num)\n","\n","        self.affine_output = nn.Linear(in_features=self.factor_num, out_features=1)\n","        self.logistic = nn.Sigmoid()\n","\n","    def forward(self, user_indices, item_indices):\n","        user_embedding = self.embedding_user(user_indices)\n","        item_embedding = self.embedding_item(item_indices)\n","        element_product = torch.mul(user_embedding, item_embedding)\n","        logits = self.affine_output(element_product)\n","        rating = self.logistic(logits)\n","        return rating.squeeze()\n","\n","    def init_weight(self):\n","        pass"]},{"cell_type":"markdown","metadata":{"id":"FSUXUsUS5OoW"},"source":["## Neural Collaborative Filtering\n","GMF learn user and item embedding separately. To empower the model it is possible to combine the features of two pathways by concatenating them and passing this concatenation to a Multi-Layer Perceptron (MLP) to learn the interaction between user and item latent features. This model is known as Neural Collaborative Filtering (NCF)\n","\n","The input to the model is userID and itemID, which is fed into an embedding layer. Thus, each user and item is given an embedding. Then there are multiple dense layers afterward, followed by a single neuron with a sigmoid activation.\n","\n","The output of the sigmoid neuron can be interpreted as the probability the user is likely to interact with an item.\n","\n","The model, we are going to implement is the following: \n","<center>  <img src=\"https://drive.google.com/uc?export=view&id=1rL_8kkHIhSlQjWr8hNal4Tyog87-2kNP\" width=\"550\" height=\"350\"> </center> \n","\n","The user in item vectors are passed to an embedding layer that build a dense or latent vectors for the sparse inputs, from the input layer. The obtained latent vectors are fed into the multi-layer neural architecture, to map the latent vectors to the predicted probability scores. The layers are responsible to find the complex user-item relations from the data. \n","\n","The output layer produces the predicted score $y_(ui)$, i.e, how much is the probability that the user u will interact with the item i.\n","\n","A pointwise loss function is used to minimize the difference between the target value Y(ui) and the corresponding predicted value.\n","\n","Formally the model we are going to implement is the following:\n","\n","$$\\mathbf{z_1} = \\phi_1 (\\mathbf{p_u}, \\mathbf{q_i}) = \\begin{bmatrix}\n","\\mathbf{p_u} \\\\ \\mathbf{q_i}\n","\\end{bmatrix}$$\n","\n","$$\\phi_L(\\mathbf{z}_{L-1}) = a_L(\\mathbf{W}_L^T\\mathbf{z}_{L-1} + \\mathbf{b}_L)$$\n","\n","$$ y_{ui} = \\sigma(\\mathbf{h}^T\\phi_L(\\mathbf{z}_{L-1}))$$\n","\n","where $\\mathbf{W}_x, \\mathbf{b}_x, a_x$ denotes the weight matrix, bias vector and activation function for the x-th layer's perceptron\n"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"sokr3I_8d5tS"},"outputs":[],"source":["class MLP(nn.Module):\n","    def __init__(self, args, num_users, num_items):\n","        super(MLP, self).__init__()\n","        self.num_users = num_users\n","        self.num_items = num_items\n","        self.factor_num = args.factor_num\n","        self.layers = args.layers\n","\n","        self.embedding_user = nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.factor_num)\n","        self.embedding_item = nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.factor_num)\n","\n","        self.fc_layers = nn.ModuleList()\n","        for idx, (in_size, out_size) in enumerate(zip(self.layers[:-1], self.layers[1:])):\n","            self.fc_layers.append(nn.Linear(in_size, out_size))\n","\n","        self.affine_output = nn.Linear(in_features=self.layers[-1], out_features=1)\n","        self.logistic = nn.Sigmoid()\n","\n","    def forward(self, user_indices, item_indices):\n","        user_embedding = self.embedding_user(user_indices)\n","        item_embedding = self.embedding_item(item_indices)\n","        vector = torch.cat([user_embedding, item_embedding], dim=-1)  # the concat latent vector\n","        for idx, _ in enumerate(range(len(self.fc_layers))):\n","            vector = self.fc_layers[idx](vector)\n","            vector = nn.ReLU()(vector)\n","            # vector = nn.BatchNorm1d()(vector)\n","            # vector = nn.Dropout(p=0.5)(vector)\n","        logits = self.affine_output(vector)\n","        rating = self.logistic(logits)\n","        return rating.squeeze()\n","\n","    def init_weight(self):\n","        pass"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1678197656765,"user":{"displayName":"Giorgio Lazzarinetti","userId":"02760930368358339868"},"user_tz":-60},"id":"TzFNcyild5y8","outputId":"43af2a80-8724-4e31-d316-6989b2e94e84"},"outputs":[{"data":{"text/plain":["_StoreAction(option_strings=['--out'], dest='out', nargs=None, const=None, default=True, type=None, choices=None, required=False, help='save model or not', metavar=None)"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["#collapse-hide\n","parser = argparse.ArgumentParser()\n","parser.add_argument(\"--seed\", \n","\ttype=int, \n","\tdefault=42, \n","\thelp=\"Seed\")\n","parser.add_argument(\"--lr\", \n","\ttype=float, \n","\tdefault=0.001, \n","\thelp=\"learning rate\")\n","parser.add_argument(\"--dropout\", \n","\ttype=float,\n","\tdefault=0.2,  \n","\thelp=\"dropout rate\")\n","parser.add_argument(\"--batch_size\", \n","\ttype=int, \n","\tdefault=256, \n","\thelp=\"batch size for training\")\n","parser.add_argument(\"--epochs\", \n","\ttype=int,\n","\tdefault=30,  \n","\thelp=\"training epoches\")\n","parser.add_argument(\"--top_k\", \n","\ttype=int, \n","\tdefault=10, \n","\thelp=\"compute metrics@top_k\")\n","parser.add_argument(\"--factor_num\", \n","\ttype=int,\n","\tdefault=32, \n","\thelp=\"predictive factors numbers in the model\")\n","parser.add_argument(\"--layers\",\n","    nargs='+', \n","    default=[64,32,16,8],\n","    help=\"MLP layers. Note that the first layer is the concatenation of user \\\n","    and item embeddings. So layers[0]/2 is the embedding size.\")\n","parser.add_argument(\"--num_ng\", \n","\ttype=int,\n","\tdefault=4, \n","\thelp=\"Number of negative samples for training set\")\n","parser.add_argument(\"--num_ng_test\", \n","\ttype=int,\n","\tdefault=100, \n","\thelp=\"Number of negative samples for test set\")\n","parser.add_argument(\"--out\", \n","\tdefault=True,\n","\thelp=\"save model or not\")"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24166,"status":"ok","timestamp":1678197623473,"user":{"displayName":"Giorgio Lazzarinetti","userId":"02760930368358339868"},"user_tz":-60},"id":"coXQgQWYd51r","outputId":"8acbc248-6083-429c-a32f-5ccce027605a"},"outputs":[],"source":["# set device and parameters\n","args = parser.parse_args(\"\")\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","# load data\n","ml_100k = pd.read_csv(\n","\tDATA_PATH, \n","\tsep=\"\\t\", \n","\tnames = ['user_id', 'item_id', 'rating', 'timestamp'], \n","\tengine='python')\n","\n","# set the num_users, items\n","num_users = ml_100k['user_id'].nunique()+1\n","num_items = ml_100k['item_id'].nunique()+1\n","\n","# construct the train and test datasets\n","data = NCF_Data(args, ml_100k)\n","train_loader = data.get_train_instance()\n","test_loader = data.get_test_instance()\n","\n","# set model and loss, optimizer\n","model = GMF(args, num_users, num_items)\n","model = model.to(device)\n","loss_function = nn.BCELoss()\n","optimizer = optim.Adam(model.parameters(), lr=args.lr)\n","\n","# train, evaluation\n","best_hr = 0\n","for epoch in range(1, args.epochs+1):\n","\tmodel.train() # Enable dropout (if have).\n","\tstart_time = time.time()\n","\n","\tfor user, item, label in train_loader:\n","\t\tuser = user.to(device)\n","\t\titem = item.to(device)\n","\t\tlabel = label.to(device)\n","\n","\t\toptimizer.zero_grad()\n","\t\tprediction = model(user, item)\n","\t\tloss = loss_function(prediction, label)\n","\t\tloss.backward()\n","\t\toptimizer.step()\n","\t\t#writer.add_scalar('loss/Train_loss', loss.item(), epoch)\n","\n","\tmodel.eval()\n","\tHR, NDCG = metrics(model, test_loader, args.top_k, device)\n","\t#writer.add_scalar('Perfomance/HR@10', HR, epoch)\n","\t#writer.add_scalar('Perfomance/NDCG@10', NDCG, epoch)\n","\n","\telapsed_time = time.time() - start_time\n","\tprint(\"Epoch {:03d}\".format(epoch) + \" time to train: \" + \n","\t\t\ttime.strftime(\"%H: %M: %S\", time.gmtime(elapsed_time)))\n","\tprint(\"HR: {:.3f}\\tNDCG: {:.3f}\".format(np.mean(HR), np.mean(NDCG)))\n","\n","\tif HR > best_hr:\n","\t\tbest_hr, best_ndcg, best_epoch = HR, NDCG, epoch\n","\t\tif args.out:\n","\t\t\tif not os.path.exists(MODEL_PATH):\n","\t\t\t\tos.mkdir(MODEL_PATH)\n","\t\t\ttorch.save(model, \n","\t\t\t\t'{}{}.pt'.format(MODEL_PATH, model.__class__.__name__))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vCebWgM1d54J"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qFWYG-w7d56r"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-WtIEPjOd588"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"73RGBr1wd5_L"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOhqRFOdF0iq0kPJOW5HrND","mount_file_id":"1OulFApVrP7ukJGQore1A7cyIVrSfq7dY","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"}},"nbformat":4,"nbformat_minor":0}
